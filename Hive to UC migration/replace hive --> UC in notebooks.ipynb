{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01c5c89f-a7a7-46a5-966b-bea780a0b7ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import base64\n",
    "import re\n",
    "import json\n",
    "\n",
    "# ==========================\n",
    "# üîß CONFIGURATION (EDIT THESE)\n",
    "# ==========================\n",
    "\n",
    "# Your Databricks workspace URL (no trailing slash)\n",
    "DATABRICKS_INSTANCE = \"https://<your-databricks-instance>.azuredatabricks.net\"\n",
    "\n",
    "# Starting workspace folder path ‚Äî script will recurse through subfolders\n",
    "WORKSPACE_PATH = \"/Workspace/Users/<your-username>@<your-domain>.com\"\n",
    "\n",
    "# Target Unity Catalog catalog name\n",
    "UC_CATALOG = \"<target_uc_catalog>\"\n",
    "\n",
    "# Authentication token\n",
    "# ‚úÖ Best practice: store this in a secret scope or environment variable\n",
    "TOKEN = dbutils.secrets.get(scope=\"<your-scope-name>\", key=\"<your-key-name>\")\n",
    "\n",
    "# HTTP headers for REST API calls\n",
    "headers = {\"Authorization\": f\"Bearer {TOKEN}\"}\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# üß† HELPER FUNCTIONS\n",
    "# ==========================\n",
    "\n",
    "def get_hive_schemas():\n",
    "    \"\"\"Fetch all Hive Metastore databases dynamically.\"\"\"\n",
    "    df = spark.sql(\"SHOW DATABASES IN hive_metastore\")\n",
    "    return [row.databaseName for row in df.collect()]\n",
    "\n",
    "\n",
    "def list_workspace_objects(path):\n",
    "    \"\"\"List all objects (directories/notebooks) recursively from a workspace path.\"\"\"\n",
    "    url = f\"{DATABRICKS_INSTANCE}/api/2.0/workspace/list\"\n",
    "    resp = requests.get(url, headers=headers, params={\"path\": path})\n",
    "    if resp.status_code == 404:\n",
    "        return []  # skip missing folders\n",
    "    resp.raise_for_status()\n",
    "    return resp.json().get(\"objects\", [])\n",
    "\n",
    "\n",
    "def export_notebook(path):\n",
    "    \"\"\"Export a Databricks notebook as source code.\"\"\"\n",
    "    url = f\"{DATABRICKS_INSTANCE}/api/2.0/workspace/export\"\n",
    "    resp = requests.get(url, headers=headers, params={\"path\": path, \"format\": \"SOURCE\"})\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    content = base64.b64decode(data[\"content\"]).decode(\"utf-8\")\n",
    "    language = data.get(\"language\", \"PYTHON\")\n",
    "    return content, language\n",
    "\n",
    "\n",
    "def import_notebook(path, content, language):\n",
    "    \"\"\"Reimport updated notebook content back into workspace.\"\"\"\n",
    "    url = f\"{DATABRICKS_INSTANCE}/api/2.0/workspace/import\"\n",
    "    encoded = base64.b64encode(content.encode(\"utf-8\")).decode(\"utf-8\")\n",
    "    payload = {\n",
    "        \"path\": path,\n",
    "        \"format\": \"SOURCE\",\n",
    "        \"language\": language,\n",
    "        \"overwrite\": True,\n",
    "        \"content\": encoded\n",
    "    }\n",
    "    resp = requests.post(url, headers=headers, json=payload)\n",
    "    resp.raise_for_status()\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# üîÑ BUILD SCHEMA MAPPINGS\n",
    "# ==========================\n",
    "\n",
    "HIVE_SCHEMAS = get_hive_schemas()\n",
    "SCHEMA_MAPPING = {schema: f\"{UC_CATALOG}.{schema}\" for schema in HIVE_SCHEMAS}\n",
    "\n",
    "print(\"Detected schema mappings:\")\n",
    "print(json.dumps(SCHEMA_MAPPING, indent=2))\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# üß© REPLACEMENT LOGIC\n",
    "# ==========================\n",
    "\n",
    "def update_table_references(content):\n",
    "    \"\"\"Replace all hive_metastore.<schema> references with UC equivalents.\"\"\"\n",
    "    updated = content\n",
    "\n",
    "    for old_schema, new_schema in SCHEMA_MAPPING.items():\n",
    "        # 1Ô∏è‚É£ Replace unquoted hive_metastore.<schema>.<table>\n",
    "        unquoted = re.compile(\n",
    "            rf\"\\b(?:hive_metastore\\s*\\.\\s*)?{re.escape(old_schema)}\\s*\\.\\s*(\\w+)\\b\",\n",
    "            flags=re.MULTILINE\n",
    "        )\n",
    "        updated = unquoted.sub(lambda m: f\"{new_schema}.{m.group(1)}\", updated)\n",
    "\n",
    "        # 2Ô∏è‚É£ Replace quoted/backticked references\n",
    "        quoted = re.compile(\n",
    "            rf\"(['\\\"`])(?:hive_metastore\\s*\\.\\s*)?{re.escape(old_schema)}\\s*\\.\\s*(\\w+)\\1\",\n",
    "            flags=re.MULTILINE\n",
    "        )\n",
    "        updated = quoted.sub(lambda m: f\"{m.group(1)}{new_schema}.{m.group(2)}{m.group(1)}\", updated)\n",
    "\n",
    "        # 3Ô∏è‚É£ Handle variable assignments like schema = \"hive_metastore.schema\"\n",
    "        assign = re.compile(\n",
    "            rf\"(\\w*_database\\s*=\\s*)([\\\"'])(?:hive_metastore\\.)?{re.escape(old_schema)}([\\\"'])\",\n",
    "            flags=re.MULTILINE\n",
    "        )\n",
    "        updated = assign.sub(rf\"\\1'{new_schema}'\", updated)\n",
    "\n",
    "    return updated\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# üöÄ WALK WORKSPACE & APPLY CHANGES\n",
    "# ==========================\n",
    "\n",
    "def update_notebooks(path):\n",
    "    \"\"\"Recursively walk workspace folders and update notebook contents.\"\"\"\n",
    "    objects = list_workspace_objects(path)\n",
    "    for obj in objects:\n",
    "        if obj[\"object_type\"] == \"NOTEBOOK\":\n",
    "            print(f\"üìù Processing notebook: {obj['path']}\")\n",
    "            content, language = export_notebook(obj[\"path\"])\n",
    "            updated = update_table_references(content)\n",
    "            if updated != content:\n",
    "                import_notebook(obj[\"path\"], updated, language)\n",
    "                print(f\"‚úÖ Updated: {obj['path']} ({language})\")\n",
    "            else:\n",
    "                print(f\" - No Hive references found in {obj['path']}\")\n",
    "        elif obj[\"object_type\"] == \"DIRECTORY\":\n",
    "            update_notebooks(obj[\"path\"])\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# üèÅ RUN MIGRATION\n",
    "# ==========================\n",
    "\n",
    "print(f\"\\nüöÄ Starting Hive ‚Üí UC reference migration in: {WORKSPACE_PATH}\")\n",
    "update_notebooks(WORKSPACE_PATH)\n",
    "print(\"\\n‚úÖ Notebook reference migration completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "replace hive --> UC in notebooks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
